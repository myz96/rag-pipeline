{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 1: Performant\n",
    "The first RAG pipeline uses best-in-class embedding and generation models, optimising for retrieval and generation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from utils import load_documents, get_nodes, create_index\n",
    "\n",
    "DOCUMENTS_PATH = \"./source_documents/\"\n",
    "DB_PATH = '../chroma_db'\n",
    "DB_COLLECTION_NAME = \"insurance_policy_collection\"\n",
    "COUNT_NODES_RETRIEVED = 2\n",
    "\n",
    "# Define Chroma client\n",
    "client = chromadb.PersistentClient(path=DB_PATH, settings=Settings(allow_reset=True))\n",
    "\n",
    "# Delete existing collection if exists\n",
    "client.reset()\n",
    "\n",
    "# Define and configure embedding and generation LLMs\n",
    "Settings.embed_model = OpenAIEmbedding() # Set embedding model globally to index and retrieve using the same model \n",
    "generation_llm = OpenAI()\n",
    "\n",
    "# Create Retriever\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "nodes = get_nodes(documents)\n",
    "chroma_collection = client.get_or_create_collection(DB_COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = create_index(nodes, vector_store)\n",
    "retriever = index.as_retriever(similarity_top_k=COUNT_NODES_RETRIEVED)\n",
    "\n",
    "# Create Query Engine\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=generation_llm,\n",
    "    similarity_top_k=COUNT_NODES_RETRIEVED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Comprehensive Plus Insurance, Comprehensive Insurance, Third Party Fire & Theft Insurance, and Third Party Property Damage Insurance are the types of insurance available.', source_nodes=[NodeWithScore(node=TextNode(id_='8cfb425c-301a-4470-8e05-675c45369e08', embedding=None, metadata={'page_label': '8', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='fe7ba71c-016d-40b5-99cf-d9f7c0010bfa', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, hash='652903d308bf4ab5335a66ab6e15afad2678aab7feef78d75fadb37e822e7f91'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='85d3d5af-bc19-4b4b-873d-f1d1808d7678', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, hash='e6a570205816c0cd5d2600079cd7b51d915f63e338bf2b4cfb0ee909b5264a9d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='135c5fa9-9ef5-4115-852e-3c54e94df66e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='38e1abe50b29b5273ac6a01fb75b8c4a553efb88b84fdb4c1f46bb8fa2047191')}, text='Motor Insurance at a glance – quick summary\\nHere’s a summary of the key details about Motor Insurance. In this Product Disclosure \\nStatement and Policy Booklet (PDS), we set out the full details about your cover and any \\nlimits, exclusions and conditions that apply.\\nUnder your policy, we cover:\\n• you\\n• anyone who has your permission to drive your vehicle\\nYou can choose from these 4 types of insurance\\n• Comprehensive Plus Insurance – our top cover\\n• Comprehensive Insurance – our standard cover\\n• Third Party Fire & Theft Insurance – our basic cover with some extras\\n• Third Party Property Damage Insurance – our basic coverType of \\ninsuranceWho we cover\\nYour vehicle and you\\n• we insure your vehicle for an agreed value or market value as shown on \\nyour current Certificate of Insurance and including:\\n –any standard equipment that comes with it, plus\\n –any modifications, options or accessories that are attached to it\\n• we insure you for claims that are made against you where the\\xa0use of your \\nvehicle causes loss or damage to someone else’s property\\nThe types of vehicles we insure include:\\n• cars  4WDs\\n• utes   vans\\n• motorcycles – which include scooters (you can only insure motorcycles \\nunder Comprehensive Insurance or Third Party Property Damage Insurance)What we  \\ninsure\\n2', start_char_idx=0, end_char_idx=1287, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6955206147595822), NodeWithScore(node=TextNode(id_='135c5fa9-9ef5-4115-852e-3c54e94df66e', embedding=None, metadata={'page_label': '9', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b1032138-7160-4808-bf56-7ad8a0ae0ae3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, hash='aa90ff63b4530159aa61861ea2a106e62a36df11cfad09c1f023b65fe17e01d5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8cfb425c-301a-4470-8e05-675c45369e08', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '8', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, hash='652903d308bf4ab5335a66ab6e15afad2678aab7feef78d75fadb37e822e7f91'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='11bce8c5-f992-4162-b1dd-3fde85db00cc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f98b434bf987feb69440d77d4f3fd20605d5cd112e993408433ca43b19032b75')}, text='Comprehensive Plus Insurance  \\n– our top cover\\n✔ Loss or damage to your vehicle\\n✔  Other benefits – up to 15 benefits (we cover you for extra things like a hire \\ncar and a taxi home after an incident we cover)\\n✔  Options you can add – you pay extra for this\\n✔ Liability cover\\n Comprehensive Insurance  \\n– our standard cover\\n✔ Loss or damage to your vehicle\\n✔ Other benefits – up to 14 benefits\\n✔ Options you can add – you pay extra for this\\n✔ Liability cover\\nThird Party Fire & Theft Insurance  \\n– our basic cover with some extras \\n✔  Loss or damage to your vehicle (fire, theft or attempted theft only)\\n✔ Other benefits – up to 7 benefits\\n✔ Liability cover\\n Third Party Property Damage Insurance  \\n– our basic cover \\n✔ Other benefits – up to 3 benefits\\n✔ Liability coverYour  \\ninsurance  \\ncoverWhen a person we cover makes a claim and is entitled to liability cover, \\nthen we will:\\n• act for them or arrange for a lawyer to represent them\\n• attempt to resolve the claim\\n• defend the claim in a court or tribunal\\nWe will decide whether to defend or resolve a claim and how much to pay to \\nresolve a claim.\\nIf we agree to cover your claim, then we will:\\n• decide whether to repair your vehicle or pay you the reasonable cost of \\nrepairs as determined by us or pay you the agreed value or market value \\nthat applies under your policy\\n• pay for any other benefits that apply to your policy\\n• pay for any options you have added to your policy\\n• deduct any amounts that apply (for example, excess, unpaid premium or \\nany unused registration and CTP insurance premium)What we pay\\nTo compare these 4 types of insurance, see the next pagepage 54\\npage 66\\npage 15\\npage 27\\npage 49page 41Also available \\nfor motorcycles\\nAlso available \\nfor motorcycles\\n3', start_char_idx=0, end_char_idx=1741, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.678513131029153)], metadata={'8cfb425c-301a-4470-8e05-675c45369e08': {'page_label': '8', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}, '135c5fa9-9ef5-4115-852e-3c54e94df66e': {'page_label': '9', 'file_name': 'nrma-car-pds-1023-east.pdf', 'file_path': '/Users/mzhao/sei/rai-assignment/rai_assignment/source_documents/nrma-car-pds-1023-east.pdf', 'file_type': 'application/pdf', 'file_size': 454985, 'creation_date': '2024-04-11', 'last_modified_date': '2024-04-09'}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\"What types of insurance are available?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreival Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval evaluation dataset does not exist. Creating one now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [14:20<00:00,  4.56s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top-2 eval</td>\n",
       "      <td>0.63172</td>\n",
       "      <td>0.553763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrievers  hit_rate       mrr\n",
       "0  top-2 eval   0.63172  0.553763"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import display_retrieval_evaluation_results, create_retrieval_qa_dataset\n",
    "\n",
    "# Allows for nested async calls in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define evaluation LLM\n",
    "evaluation_llm = OpenAI(temperature=0, model=\"gpt-4\") \n",
    "\n",
    "# Create QA dataset\n",
    "QA_DATASET_PATH = \"./qa_datasets/qa_dataset_1.json\"\n",
    "qa_dataset = create_retrieval_qa_dataset(nodes, evaluation_llm, QA_DATASET_PATH)\n",
    "\n",
    "# Evaluate QA dataset\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_retrieval_evaluation_results(f\"top-{COUNT_NODES_RETRIEVED} eval\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Plus Insurance, Comprehensive Insurance, Third Party Fire & Theft Insurance, and Third Party Property Damage Insurance are the types of insurance available.\n",
      "Faithfulness: True\n",
      "Relevance: 1.0\n",
      "Context: 1.0\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator, AnswerRelevancyEvaluator, ContextRelevancyEvaluator\n",
    "\n",
    "# Define evaluation LLM\n",
    "evaluation_llm = OpenAI() # Ideally this should be a superior model to generation_llm (e.g. GPT-4), however due to rate limits on lower usage tiers, GPT-3.5 is used\n",
    "\n",
    "# Example query and response \n",
    "query = \"What types of insurance are available?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n",
    "\n",
    "# Example Faithfulness evaluation\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=evaluation_llm)\n",
    "eval_result = faithfulness_evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"Faithfulness: \" + str(eval_result.passing))\n",
    "\n",
    "# Example Relevancy evaluation\n",
    "relevancy_evaluator = AnswerRelevancyEvaluator(llm=evaluation_llm)\n",
    "eval_result = relevancy_evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"Relevance: \" + str(eval_result.score))\n",
    "\n",
    "# Example Context evaluation\n",
    "context_evaluator = ContextRelevancyEvaluator(llm=evaluation_llm)\n",
    "eval_result = context_evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"Context: \" + str(eval_result.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    create_question_dataset, \n",
    "    create_prediction_dataset, \n",
    "    create_judges, \n",
    "    create_evaluation_tasks, \n",
    "    evaluate_tasks, \n",
    "    display_generation_evaluation_results\n",
    ")\n",
    "\n",
    "# Create rag question dataset    \n",
    "rag_dataset = create_question_dataset(nodes, evaluation_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:05<00:00,  1.87it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:04<00:00,  2.01it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:04<00:00,  2.20it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:04<00:00,  2.12it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:05<00:00,  1.85it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:04<00:02,  1.87it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9208123997386777 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59752, Requested 1135. Please try again in 887ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  60%|██████    | 6/10 [00:05<00:02,  1.36it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5218989664610422 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59586, Requested 1189. Please try again in 775ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6904741819289825 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59567, Requested 1197. Please try again in 764ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it]\n",
      "Batch processing of predictions:  70%|███████   | 7/10 [00:05<00:02,  1.49it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5636980485675342 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59718, Requested 1181. Please try again in 899ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:04<00:02,  1.75it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.21252317779865315 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59677, Requested 1230. Please try again in 907ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  70%|███████   | 7/10 [00:04<00:01,  2.51it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.36185038477493536 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59530, Requested 1359. Please try again in 889ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:03<00:02,  2.09it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.17774115987396488 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59752, Requested 728. Please try again in 480ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5231388374255569 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59154, Requested 1305. Please try again in 459ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.16948163593769772 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59314, Requested 1104. Please try again in 418ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:04<00:04,  1.42it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3711351171386895 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59741, Requested 930. Please try again in 671ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7772759168378854 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59729, Requested 1002. Please try again in 731ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.2720251650624872 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59646, Requested 1068. Please try again in 714ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:04<00:04,  1.23it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7744649617937317 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59576, Requested 1215. Please try again in 791ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.09it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:04<00:03,  1.46it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6764171106610086 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59648, Requested 1173. Please try again in 821ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.32889418604247456 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59603, Requested 1274. Please try again in 877ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.22963929071094713 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59595, Requested 1270. Please try again in 865ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.03it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:05<00:04,  1.18it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.15376227436261025 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59519, Requested 1107. Please try again in 626ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  60%|██████    | 6/10 [00:05<00:02,  1.63it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.1901100291034249 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59488, Requested 1189. Please try again in 677ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.03126640524427293 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59436, Requested 1217. Please try again in 653ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.04it/s]\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:04<00:04,  1.24it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.34042665482159307 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59614, Requested 990. Please try again in 604ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.36180963887297923 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59450, Requested 1190. Please try again in 640ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  60%|██████    | 6/10 [00:05<00:02,  1.96it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4266359700852037 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59596, Requested 1122. Please try again in 718ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:04<00:05,  1.20it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.379482468515675 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59628, Requested 1071. Please try again in 699ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4475739948041698 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59578, Requested 1123. Please try again in 701ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8143018502822293 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59570, Requested 1115. Please try again in 685ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6213243467058563 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59402, Requested 1269. Please try again in 671ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:11<00:00,  1.12s/it]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:04<00:02,  1.82it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.673819502622506 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59697, Requested 1110. Please try again in 807ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.13795448046931635 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59530, Requested 1293. Please try again in 823ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:03<00:04,  1.30it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.1077727366500093 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59571, Requested 1115. Please try again in 686ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.027603077940892784 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59535, Requested 1132. Please try again in 667ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5256371813792645 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59514, Requested 1163. Please try again in 677ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:04<00:02,  1.92it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.331599488812923 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59627, Requested 1028. Please try again in 655ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7879897563045516 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59589, Requested 1081. Please try again in 670ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4802922576628883 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59491, Requested 1178. Please try again in 669ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:10<00:00,  1.02s/it]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:06<00:00,  1.55it/s]\n",
      "Batch processing of predictions:  10%|█         | 1/10 [00:03<00:32,  3.58s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9927149198224196 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59743, Requested 884. Please try again in 627ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  20%|██        | 2/10 [00:04<00:16,  2.08s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6379943685360245 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59519, Requested 998. Please try again in 517ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.21it/s]\n",
      "Batch processing of predictions:  20%|██        | 2/10 [00:05<00:19,  2.40s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5048591613706226 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59667, Requested 1081. Please try again in 748ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.45037673108690357 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59650, Requested 1107. Please try again in 757ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5909161529513661 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59660, Requested 1082. Please try again in 742ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.01it/s]\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:04<00:04,  1.46it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7584847596778731 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59610, Requested 1238. Please try again in 848ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  60%|██████    | 6/10 [00:05<00:02,  1.71it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7166843218693306 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59525, Requested 1319. Please try again in 844ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6173360453457033 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59469, Requested 1383. Please try again in 852ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:04<00:03,  1.32it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.18471761014378896 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59850, Requested 1227. Please try again in 1.077s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  70%|███████   | 7/10 [00:05<00:01,  1.81it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5960205500094421 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59725, Requested 1206. Please try again in 931ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n",
      "Batch processing of predictions:  20%|██        | 2/10 [00:05<00:19,  2.38s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.02618528475197479 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59263, Requested 1297. Please try again in 560ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4744723878329895 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59202, Requested 1340. Please try again in 542ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8260333109552997 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59177, Requested 1376. Please try again in 553ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  40%|████      | 4/10 [00:05<00:05,  1.00it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6812899120072076 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 58793, Requested 1348. Please try again in 141ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n",
      "Batch processing of predictions:  30%|███       | 3/10 [00:03<00:06,  1.06it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.619047052065423 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59614, Requested 720. Please try again in 334ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  70%|███████   | 7/10 [00:04<00:01,  2.93it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.40503983925898157 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59339, Requested 1122. Please try again in 461ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:07<00:00,  1.33it/s]\n",
      "Batch processing of predictions:  60%|██████    | 6/10 [00:04<00:02,  1.97it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.721547168621275 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59811, Requested 890. Please try again in 701ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7043821289582822 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59693, Requested 1035. Please try again in 728ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:08<00:00,  1.21it/s]\n",
      "Batch processing of predictions:  60%|██████    | 6/10 [00:05<00:02,  1.57it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5236330689872574 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59568, Requested 1156. Please try again in 724ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9640045878070018 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59476, Requested 1254. Please try again in 730ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.22375100727355324 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59422, Requested 1318. Please try again in 740ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n",
      "Batch processing of predictions:  30%|███       | 3/10 [00:04<00:07,  1.06s/it]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9347126078703534 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59571, Requested 1107. Please try again in 678ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9137478226625568 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59418, Requested 1246. Please try again in 664ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions:  50%|█████     | 5/10 [00:05<00:03,  1.27it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.7798765722116008 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59704, Requested 1176. Please try again in 880ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n",
      "Batch processing of predictions: 100%|██████████| 10/10 [00:06<00:00,  1.51it/s]\n",
      "Batch processing of predictions: 100%|██████████| 8/8 [00:04<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create prediction dataset\n",
    "prediction_data = await create_prediction_dataset(rag_dataset, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation tasks using evaluation LLM judge\n",
    "judges = create_judges(evaluation_llm)\n",
    "eval_tasks = create_evaluation_tasks(rag_dataset, prediction_data, judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5509140349338266 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59753, Requested 1330. Please try again in 1.083s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9820748420327329 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59839, Requested 1213. Please try again in 1.052s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.36309014559659203 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59544, Requested 1385. Please try again in 929ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9226496503724566 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59660, Requested 1396. Please try again in 1.056s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.4544256780734628 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59651, Requested 1446. Please try again in 1.097s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.9861997148449807 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59590, Requested 1195. Please try again in 785ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3387829176920576 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-M4tHPAmLSbtutEWD7DSkmcHM on tokens per min (TPM): Limit 60000, Used 59421, Requested 1333. Please try again in 754ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 0.8690692260201004 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 0.13826530951698213 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 1.68361928695386 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 1.1631645159695436 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 3.503745514102728 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 3.233100972302536 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 5.547320121641324 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 0.6682541164915703 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 12.532355329694939 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n",
      "Retrying llama_index.embeddings.openai.base.aget_embedding in 10.417962230086944 seconds as it raised BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate tasks to get evaluation results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluate_tasks(eval_tasks)\n",
      "File \u001b[0;32m~/sei/rai-assignment/rai_assignment/utils.py:154\u001b[0m, in \u001b[0;36mevaluate_tasks\u001b[0;34m(eval_tasks)\u001b[0m\n\u001b[1;32m    152\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunked(eval_tasks, EVALUATION_BATCH_SIZE):\n\u001b[0;32m--> 154\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mchunk, mininterval\u001b[38;5;241m=\u001b[39mMINIMUM_INTERVAL, delay\u001b[38;5;241m=\u001b[39mDELAY)\n\u001b[1;32m    155\u001b[0m     eval_results\u001b[38;5;241m.\u001b[39mextend(eval_result)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_results\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather\u001b[0;34m(cls, loop, timeout, total, *fs, **tqdm_kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:79\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n\u001b[1;32m     78\u001b[0m ifs \u001b[38;5;241m=\u001b[39m [wrap_awaitable(i, f) \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fs)]\n\u001b[0;32m---> 79\u001b[0m res \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mas_completed(ifs, loop\u001b[38;5;241m=\u001b[39mloop, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     80\u001b[0m                                          total\u001b[38;5;241m=\u001b[39mtotal, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtqdm_kwargs)]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m _, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(res)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/asyncio/tasks.py:615\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tqdm/asyncio.py:76\u001b[0m, in \u001b[0;36mtqdm_asyncio.gather.<locals>.wrap_awaitable\u001b[0;34m(i, f)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_awaitable\u001b[39m(i, f):\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, \u001b[38;5;28;01mawait\u001b[39;00m f\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/semantic_similarity.py:76\u001b[0m, in \u001b[0;36mSemanticSimilarityEvaluator.aevaluate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify both response and reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m response_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39maget_text_embedding(response)\n\u001b[0;32m---> 76\u001b[0m reference_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39maget_text_embedding(reference)\n\u001b[1;32m     78\u001b[0m similarity_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_similarity_fn(response_embedding, reference_embedding)\n\u001b[1;32m     79\u001b[0m passing \u001b[38;5;241m=\u001b[39m similarity_score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_similarity_threshold\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:307\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.async_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    304\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:280\u001b[0m, in \u001b[0;36mBaseEmbedding.aget_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    272\u001b[0m dispatch_event(\n\u001b[1;32m    273\u001b[0m     EmbeddingStartEvent(\n\u001b[1;32m    274\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    278\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[1;32m    279\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 280\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_text_embedding(text)\n\u001b[1;32m    282\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    283\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    284\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [text],\n\u001b[1;32m    285\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [text_embedding],\n\u001b[1;32m    286\u001b[0m         }\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m dispatch_event(\n\u001b[1;32m    289\u001b[0m     EmbeddingEndEvent(\n\u001b[1;32m    290\u001b[0m         chunks\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m    291\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[text_embedding],\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m )\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:412\u001b[0m, in \u001b[0;36mOpenAIEmbedding._aget_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously get text embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m    411\u001b[0m aclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_aclient()\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m aget_embedding(\n\u001b[1;32m    413\u001b[0m     aclient,\n\u001b[1;32m    414\u001b[0m     text,\n\u001b[1;32m    415\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[1;32m    417\u001b[0m )\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:157\u001b[0m, in \u001b[0;36maget_embedding\u001b[0;34m(aclient, text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Asynchronously get embedding.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mNOTE: Copied from OpenAI's embedding utils:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 157\u001b[0m     (\u001b[38;5;28;01mawait\u001b[39;00m aclient\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m[text], model\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;241m.\u001b[39membedding\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/openai/resources/embeddings.py:214\u001b[0m, in \u001b[0;36mAsyncEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    208\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    209\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    216\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[1;32m    217\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    218\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    219\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    220\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    221\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    222\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    223\u001b[0m     ),\n\u001b[1;32m    224\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[1;32m    225\u001b[0m )\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/openai/_base_client.py:1743\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1731\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1739\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1740\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1741\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1742\u001b[0m     )\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/openai/_base_client.py:1446\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1438\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1439\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1445\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1447\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1448\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1449\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1450\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1451\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1452\u001b[0m     )\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/openai/_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1536\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1540\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1541\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1544\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1545\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# Evaluate tasks to get evaluation results\n",
    "eval_results = await evaluate_tasks(eval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "display_generation_evaluation_results(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
