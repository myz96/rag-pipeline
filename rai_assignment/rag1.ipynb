{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 1: Performant\n",
    "The first RAG pipeline uses best-in-class embedding and generation models, optimising for retrieval and generation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from utils import load_documents, get_nodes, create_index\n",
    "\n",
    "DOCUMENTS_PATH = \"./source_documents/\"\n",
    "DB_PATH = '../chroma_db'\n",
    "DB_COLLECTION_NAME = \"insurance_policy_collection\"\n",
    "COUNT_NODES_RETRIEVED = 2\n",
    "\n",
    "# Define Chroma client\n",
    "client = chromadb.PersistentClient(path=DB_PATH, settings=Settings(allow_reset=True))\n",
    "\n",
    "# Delete existing collection if exists\n",
    "client.reset()\n",
    "\n",
    "# Define and configure embedding and generation LLMs\n",
    "Settings.embed_model = OpenAIEmbedding() # Set embedding model globally to index and retrieve using the same model \n",
    "generation_llm = OpenAI()\n",
    "\n",
    "# Create Retriever\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "nodes = get_nodes(documents)\n",
    "chroma_collection = client.get_or_create_collection(DB_COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = create_index(nodes, vector_store)\n",
    "retriever = index.as_retriever(similarity_top_k=COUNT_NODES_RETRIEVED)\n",
    "\n",
    "# Create Query Engine\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=generation_llm,\n",
    "    similarity_top_k=COUNT_NODES_RETRIEVED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Plus Insurance, Comprehensive Insurance, Third Party Fire & Theft Insurance, and Third Party Property Damage Insurance are the types of insurance available.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What types of insurance are available?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreival Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous retrieval evaluation dataset deleted successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:57<00:00,  1.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top-2 eval</td>\n",
       "      <td>0.71134</td>\n",
       "      <td>0.641753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrievers  hit_rate       mrr\n",
       "0  top-2 eval   0.71134  0.641753"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from utils import display_retrieval_evaluation_results, create_retrieval_qa_dataset\n",
    "\n",
    "# Allows for nested async calls in Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define evaluation LLM\n",
    "evaluation_llm = OpenAI(temperature=0) # Ideally this should be a superior model to generation_llm (e.g. GPT-4), however due costs and rate limits, GPT-3.5 is used\n",
    "\n",
    "# Create QA dataset\n",
    "QA_DATASET_PATH = \"./qa_datasets/qa_dataset_1.json\"\n",
    "qa_dataset = create_retrieval_qa_dataset(nodes, evaluation_llm, QA_DATASET_PATH)\n",
    "\n",
    "# Evaluate QA dataset\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_retrieval_evaluation_results(f\"top-{COUNT_NODES_RETRIEVED} eval\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Plus Insurance, Comprehensive Insurance, Third Party Fire & Theft Insurance, and Third Party Property Damage Insurance are the types of insurance available.\n",
      "Faithfulness: True\n",
      "Relevance: 1.0\n",
      "Context: 1.0\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator, AnswerRelevancyEvaluator, ContextRelevancyEvaluator\n",
    "\n",
    "# Example query and response \n",
    "query = \"What types of insurance are available?\"\n",
    "response = query_engine.query(query)\n",
    "print(response)\n",
    "\n",
    "# Example Faithfulness evaluation\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=evaluation_llm)\n",
    "eval_result = faithfulness_evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"Faithfulness: \" + str(eval_result.passing))\n",
    "\n",
    "# Example Relevancy evaluation\n",
    "relevancy_evaluator = AnswerRelevancyEvaluator(llm=evaluation_llm)\n",
    "eval_result = relevancy_evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"Relevance: \" + str(eval_result.score))\n",
    "\n",
    "# Example Context evaluation\n",
    "context_evaluator = ContextRelevancyEvaluator(llm=evaluation_llm)\n",
    "eval_result = context_evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"Context: \" + str(eval_result.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    create_question_dataset, \n",
    "    create_prediction_dataset, \n",
    "    create_judges, \n",
    "    create_evaluation_tasks, \n",
    "    evaluate_tasks, \n",
    "    display_generation_evaluation_results\n",
    ")\n",
    "\n",
    "# Create rag question dataset    \n",
    "rag_dataset = create_question_dataset(nodes, evaluation_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch processing of predictions: 100%|██████████| 5/5 [00:18<00:00,  3.80s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.57s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.54s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.44s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.67s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.44s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.56s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.51s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:14<00:00,  2.86s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.44s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:11<00:00,  2.40s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.51s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.61s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:14<00:00,  2.84s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.57s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.56s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.60s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.71s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.54s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.51s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:16<00:00,  3.37s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.60s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.42s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.52s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.59s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.67s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:20<00:00,  4.01s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.43s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.70s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.60s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.47s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.68s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.65s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:13<00:00,  2.70s/it]\n",
      "Batch processing of predictions: 100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\n",
      "Batch processing of predictions: 100%|██████████| 2/2 [00:11<00:00,  5.86s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create prediction dataset\n",
    "prediction_data = await create_prediction_dataset(rag_dataset, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation tasks using evaluation LLM judge\n",
    "judges = create_judges(evaluation_llm)\n",
    "eval_tasks = create_evaluation_tasks(rag_dataset, prediction_data, judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tasks to get evaluation results\n",
    "eval_results = await evaluate_tasks(eval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>mean value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.986631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.883065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_semantic_similarity_score</th>\n",
       "      <td>0.944905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                             mean value\n",
       "metrics                                   \n",
       "mean_answer_relevancy_score       0.986631\n",
       "mean_context_relevancy_score      0.883065\n",
       "mean_faithfulness_score           0.909091\n",
       "mean_semantic_similarity_score    0.944905"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display evaluation results\n",
    "display_generation_evaluation_results(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
