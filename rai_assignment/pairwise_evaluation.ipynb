{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise RAG pipeline evaluation\n",
    "E2E recommendation comparing both RAG pipelines \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG1: Performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An instance of Chroma already exists for ../chroma_db with different settings",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m COUNT_NODES_RETRIEVED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Define Chroma client\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mchromadb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPersistentClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDB_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Delete existing collection if exists\u001b[39;00m\n\u001b[1;32m     18\u001b[0m client\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/chromadb/__init__.py:146\u001b[0m, in \u001b[0;36mPersistentClient\u001b[0;34m(path, settings, tenant, database)\u001b[0m\n\u001b[1;32m    143\u001b[0m tenant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(tenant)\n\u001b[1;32m    144\u001b[0m database \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(database)\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClientCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/chromadb/api/client.py:139\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     tenant: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_TENANT,\n\u001b[1;32m    136\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    137\u001b[0m     settings: Settings \u001b[38;5;241m=\u001b[39m Settings(),\n\u001b[1;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant \u001b[38;5;241m=\u001b[39m tenant\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase \u001b[38;5;241m=\u001b[39m database\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/chromadb/api/client.py:43\u001b[0m, in \u001b[0;36mSharedSystemClient.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     40\u001b[0m     settings: Settings \u001b[38;5;241m=\u001b[39m Settings(),\n\u001b[1;32m     41\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_identifier \u001b[38;5;241m=\u001b[39m SharedSystemClient\u001b[38;5;241m.\u001b[39m_get_identifier_from_settings(settings)\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mSharedSystemClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_system_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sei/rai-assignment/.venv/lib/python3.11/site-packages/chromadb/api/client.py:62\u001b[0m, in \u001b[0;36mSharedSystemClient._create_system_if_not_exists\u001b[0;34m(cls, identifier, settings)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# For now, the settings must match\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_system\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;241m!=\u001b[39m settings:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of Chroma already exists for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with different settings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_identifer_to_system[identifier]\n",
      "\u001b[0;31mValueError\u001b[0m: An instance of Chroma already exists for ../chroma_db with different settings"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from utils import load_documents, get_nodes, create_index\n",
    "\n",
    "DOCUMENTS_PATH = \"./source_documents\" \n",
    "DB_PATH = '../chroma_db'\n",
    "DB_COLLECTION_NAME = \"insurance_policy_collection\" \n",
    "COUNT_NODES_RETRIEVED = 2\n",
    "\n",
    "# Define Chroma client\n",
    "client = chromadb.PersistentClient(path=DB_PATH, settings=Settings(allow_reset=True))\n",
    "\n",
    "# Delete existing collection if exists\n",
    "client.reset()\n",
    "\n",
    "# Define and configure embedding and generation LLMs\n",
    "Settings.embed_model = OpenAIEmbedding() # Set embedding model globally to index and retrieve using the same model \n",
    "generation_llm = OpenAI()\n",
    "\n",
    "# Create Retriever\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "nodes = get_nodes(documents)\n",
    "chroma_collection = client.get_or_create_collection(DB_COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = create_index(nodes, vector_store)\n",
    "retriever = index.as_retriever(similarity_top_k=COUNT_NODES_RETRIEVED)\n",
    "\n",
    "# Create Query Engine\n",
    "query_engine_1 = index.as_query_engine(\n",
    "    llm=generation_llm,\n",
    "    similarity_top_k=COUNT_NODES_RETRIEVED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG2: Fast and free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09ec83865b54265a7b43ec7d15fc128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e9533c02054918a89f208adc3b4982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import weaviate\n",
    "from utils import load_documents, get_nodes, create_index\n",
    "\n",
    "DOCUMENTS_PATH = \"./source_documents/\"\n",
    "INDEX_NAME = \"InsurancePolicyIndex\" \n",
    "COUNT_NODES_RETRIEVED = 2\n",
    "DENSE_VECTOR_HYBRID_WEIGHTING = 0.5 \n",
    "\n",
    "# Define Weaviate client \n",
    "client = weaviate.Client(embedded_options=weaviate.EmbeddedOptions())\n",
    "\n",
    "# Delete existing index if exists\n",
    "client.schema.delete_class(INDEX_NAME)\n",
    "\n",
    "# Define and configure embedding and generation LLMs\n",
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\") # Set embedding model globally to index and retrieve using the same model \n",
    "generation_llm  = Ollama(model=\"gemma:2b\", request_timeout=30.0)\n",
    "\n",
    "# Create Retriever\n",
    "documents = load_documents(DOCUMENTS_PATH)\n",
    "nodes = get_nodes(documents)\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, index_name=INDEX_NAME)\n",
    "index = create_index(nodes, vector_store)\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "# Configure Query Engine\n",
    "query_engine_2 = index.as_query_engine(\n",
    "    vector_store_query_mode = \"hybrid\", \n",
    "    alpha = DENSE_VECTOR_HYBRID_WEIGHTING,\n",
    "    llm = generation_llm,\n",
    "    similarity_top_k = COUNT_NODES_RETRIEVED, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # define jupyter display function\n",
    "# def display_pairwise_eval_df(query, response1, response2, eval_result) -> None:\n",
    "#     eval_df = pd.DataFrame(\n",
    "#         {\n",
    "#             \"Query\": query,\n",
    "#             \"Reference Response (Answer 1)\": response2,\n",
    "#             \"Current Response (Answer 2)\": response1,\n",
    "#             \"Score\": eval_result.score,\n",
    "#             \"Reason\": eval_result.feedback,\n",
    "#         },\n",
    "#         index=[0],\n",
    "#     )\n",
    "#     eval_df = eval_df.style.set_properties(\n",
    "#         **{\n",
    "#             \"inline-size\": \"300px\",\n",
    "#             \"overflow-wrap\": \"break-word\",\n",
    "#         },\n",
    "#         subset=[\"Current Response (Answer 2)\", \"Reference Response (Answer 1)\"]\n",
    "#     )\n",
    "#     display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c17cf_row0_col1, #T_c17cf_row0_col2 {\n",
       "  inline-size: 300px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c17cf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c17cf_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_c17cf_level0_col1\" class=\"col_heading level0 col1\" >Reference Response (Answer 1)</th>\n",
       "      <th id=\"T_c17cf_level0_col2\" class=\"col_heading level0 col2\" >Current Response (Answer 2)</th>\n",
       "      <th id=\"T_c17cf_level0_col3\" class=\"col_heading level0 col3\" >Score</th>\n",
       "      <th id=\"T_c17cf_level0_col4\" class=\"col_heading level0 col4\" >Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c17cf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c17cf_row0_col0\" class=\"data row0 col0\" >What are the problems with HN?</td>\n",
       "      <td id=\"T_c17cf_row0_col1\" class=\"data row0 col1\" >The problem with HN is that it was a major source of stress and a 60% chance of being the source of HN's problems.</td>\n",
       "      <td id=\"T_c17cf_row0_col2\" class=\"data row0 col2\" >The issues with HN were that it caused a significant amount of stress for the individual involved, making it the biggest source of stress in their work. This stress was not directly related to the core work of selecting and helping founders, which made it a source of frustration and distraction.</td>\n",
       "      <td id=\"T_c17cf_row0_col3\" class=\"data row0 col3\" >1.000000</td>\n",
       "      <td id=\"T_c17cf_row0_col4\" class=\"data row0 col4\" >Assistant A provides a more detailed and coherent response to the user's question. It explains that the problems with HN were related to the stress it caused, which was not directly related to the core work of selecting and helping founders, making it a source of frustration and distraction. On the other hand, Assistant B's response is less clear and seems to contain a statistical error or misunderstanding (\"a 60% chance of being the source of HN's problems\"). Therefore, Assistant A's response is more helpful, accurate, and detailed. \n",
       "\n",
       "Final Verdict: [[A]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x52b03fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n",
    "from utils import display_pairwise_eval_df\n",
    "\n",
    "# Define evaluation LLM\n",
    "evaluation_llm = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "# Example Queries\n",
    "query = \"What insurances are available?\"\n",
    "response1 = str(query_engine_1.query(query))\n",
    "response2 = str(query_engine_2.query(query))\n",
    "\n",
    "# Evaluate pair of responses\n",
    "pairwise_evaluator = PairwiseComparisonEvaluator(llm=evaluation_llm)\n",
    "eval_result = await pairwise_evaluator.aevaluate(\n",
    "    query=query, response=response1, second_response=response2\n",
    ")\n",
    "\n",
    "display_pairwise_eval_df(query, response1, response2, eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    create_question_dataset, \n",
    "    create_prediction_dataset, \n",
    "    evaluate_tasks, \n",
    ")\n",
    "\n",
    "# Create rag question dataset    \n",
    "rag_dataset = create_question_dataset(nodes, evaluation_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction datasets\n",
    "prediction_data_1 = await create_prediction_dataset(rag_dataset, query_engine_1)\n",
    "prediction_data_2 = await create_prediction_dataset(rag_dataset, query_engine_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation tasks using evaluation LLM judge\n",
    "eval_tasks = []\n",
    "for example, prediction_1, prediction_2 in zip(\n",
    "    rag_dataset.examples, prediction_data_1.predictions, prediction_data_2.predictions\n",
    "):\n",
    "    eval_tasks.append(\n",
    "        PairwiseComparisonEvaluator(llm=evaluation_llm).aevaluate(\n",
    "            query=query, \n",
    "            response=prediction_1, \n",
    "            second_response=prediction_2\n",
    "            sleep_time_in_seconds=1.5,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tasks to get evaluation results\n",
    "eval_results = await evaluate_tasks(eval_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rag</th>\n",
       "      <th>mean value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metrics</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_answer_relevancy_score</th>\n",
       "      <td>0.864865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_context_relevancy_score</th>\n",
       "      <td>0.728041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_faithfulness_score</th>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_semantic_similarity_score</th>\n",
       "      <td>0.924617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "rag                             mean value\n",
       "metrics                                   \n",
       "mean_answer_relevancy_score       0.864865\n",
       "mean_context_relevancy_score      0.728041\n",
       "mean_faithfulness_score           0.513514\n",
       "mean_semantic_similarity_score    0.924617"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation.notebook_utils import get_eval_results_df\n",
    "\n",
    "# Viewing evaluation results\n",
    "_, mean_pairwise_df = get_eval_results_df(\n",
    "    [\"mean value\"] * len(eval_results),\n",
    "    eval_results,\n",
    "    metric=\"pairwise\",\n",
    ")\n",
    "\n",
    "mean_scores_df = pd.concat(\n",
    "    [\n",
    "        mean_pairwise_df.reset_index(),\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")\n",
    "mean_scores_df = mean_scores_df.set_index(\"index\")\n",
    "mean_scores_df.index = mean_scores_df.index.set_names([\"metrics\"])\n",
    "mean_scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
